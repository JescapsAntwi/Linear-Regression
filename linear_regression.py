# -*- coding: utf-8 -*-
"""Linear_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A10jlY77yRVNVfFD5m3nZVANVVAiIpCi


This code implements a simple linear regression model from scratch using gradient descent optimization. Let me break down its key components and functionality:

Data Setup and Initial Prediction:
Creates training data with two arrays: x_train (distances) and y_train (prices)
Implements a linear prediction function predict(x,m,c) that calculates y = mx + c
Initially sets arbitrary values for slope (m) and intercept (c)
Plots the training data points and initial prediction line using matplotlib
Cost Function:
Implements Mean Squared Error (MSE) cost function through cost(y,y_hat)
Calculates the average squared difference between actual values (y) and predicted values (y_hat)
The division by 2 is a common convention to simplify the derivative calculation
Gradient Descent Implementation:
p_d(y,x,m,c) calculates partial derivatives with respect to m (slope) and c (intercept)
grad_des(y_train,x_train,m,c) implements gradient descent optimization:
Uses learning rate Î± (alpha) = 0.0006
Runs for 100 iterations
Updates m and c using the gradient descent update rule
Prints the cost at each iteration to show convergence
Returns the optimized values of m and c
The code essentially fits a straight line to the given data points by:

Starting with initial guesses for slope and intercept
Calculating how wrong these guesses are (cost function)
Adjusting the parameters in the direction that reduces the error (gradient descent)
Repeating this process until it finds the best-fitting line
This is a fundamental example of machine learning, specifically demonstrating how linear regression works under the hood without using high-level libraries like scikit-learn. The visualization helps show how the line fits through the data points.

The specific dataset appears to be modeling a relationship between distance (x) and price (y), possibly representing something like housing prices based on distance from a city center or similar real-world relationship.

"""

import numpy as np

x_train = np.array([10,20,50,60,100])
y_train = np.array([1000,1200,1500,1800,2000])

print(f"distance {x_train}")
print(f"price {y_train}")

#y_hat = mx+c
def predict(x,m,c):
  out = np.zeros(x.shape[0])
  for i in range(len(out)):
    out[i] = m*x[i] + c
  return out

m = 15
c = 800
m = 50
c =2
hyp = predict(x_train,m,c)
print(f"hypothesised values {hyp}")

import matplotlib.pyplot as plt

plt.scatter(x_train,y_train,marker = "*", color = "r")
plt.plot(x_train,hyp,color = "b")
plt.show()

def cost(y,y_hat):
  costs = np.zeros(y.shape[0])
  for i in range (len(costs)):
    costs[i] = (y[i] - y_hat[i])**2
  return sum(costs)/(2*len(costs))

cost(y_train,hyp)

#w = w-a
def p_d(y,x,m,c):
  error_m = np.zeros(y.shape[0])
  error_c = np.zeros(y.shape[0])
  dj_dm = 0
  dj_dc = 0
  y_hat = predict(x,m,c)
  print(f"y_hat {y_hat}")
  for i in range (error_m.shape[0]):
    error_m[i] = (y[i] - y_hat[i])*x[i]
    error_c[i] = (y[i] - y_hat[i])
  # print(f"errorc {error_c}")
  # print(f"errorm {error_m}")
  dj_dm = -sum(error_m)/len(error_m)
  dj_dc = -sum(error_c)/len(error_c)

  return dj_dm,dj_dc
print(m)
print(c)
dm,dc =p_d(y_train,x_train,m,c)
print(dm)
print(dc)


def grad_des(y_train,x_train,m,c):
    a = 0.0006
    for i in range(100):
        dj_dm,dj_dc = p_d(y_train,x_train,m,c)
        m = m + a*dj_dm
        c = c + a*dj_dc
        print(f"cost {cost(y_train,predict(x_train,m,c))}")
    return m,c
m,c = grad_des(y_train,x_train,m,c)

print(f"m {m}")
print(f"c {c}")

def hypothesis(w, x, b):
    return w * x + b

def cost_j(y_hat, y):
    return np.sum((y_hat - y) ** 2) / (2 * len(y))

def deriv(w, b, x, y):
    m = len(y)
    y_hat = hypothesis(w, x, b)
    dj_dw = -np.sum((y - y_hat) * x) / m
    dj_db = -np.sum(y - y_hat) / m
    return dj_dw, dj_db

def gradient_desc(x_train, y_train, w, b, alpha, num_iter):
    for i in range(num_iter):
        hyp = hypothesis(w, x_train, b)
        costf = cost_j(hyp, y_train)
        dj_dw, dj_db = deriv(w, b, x_train, y_train)
        w = w - (alpha * dj_dw)
        b = b - (alpha * dj_db)
        print(f"Iteration {i+1}: Cost {costf}, w {w}, b {b}")
    return w, b

# Example usage
w_initial = 0
b_initial = 0
alpha = 0.0006
num_iter = 100

w, b = gradient_desc(x_train, y_train, w_initial, b_initial, alpha, num_iter)

